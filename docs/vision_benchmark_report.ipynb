{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Caeliai Genesis 01: The Vision Benchmark - Technical Report\n",
        "\n",
        "## Abstract\n",
        "\n",
        "This report presents the results of comprehensive benchmarking tests designed to evaluate vision models beyond simple accuracy metrics. The Caeliai Vision Benchmark focuses on three critical aspects: uncertainty detection, aesthetic cohesion understanding, and robust brand identification. Our testing methodology evaluates how well models can distinguish target images from noise, maintain coherence within aesthetic collections, and demonstrate appropriate uncertainty when encountering ambiguous cases.\n",
        "\n",
        "The benchmark tests three state-of-the-art vision models (CLIP, SigLIP, and DINOv2) across multiple test suites, providing insights into their real-world performance for fashion and design applications where nuanced visual understanding is paramount.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_combine_benchmark_data():\n",
        "    \"\"\"\n",
        "    Load and combine data from both benchmark JSON files into a unified DataFrame.\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: Combined data with columns for model, test_type, and various metrics\n",
        "    \"\"\"\n",
        "    \n",
        "    # Load tier1 results (gauntlet and cohesion tests)\n",
        "    with open('data/tier1_results_20250627_204655.json', 'r') as f:\n",
        "        tier1_data = json.load(f)\n",
        "    \n",
        "    # Load designer noise results\n",
        "    with open('data/designer_noise_results_20250627_224305.json', 'r') as f:\n",
        "        noise_data = json.load(f)\n",
        "    \n",
        "    all_results = []\n",
        "    \n",
        "    # Process tier1 data (gauntlet and cohesion tests)\n",
        "    for model_name, model_data in tier1_data['results'].items():\n",
        "        for test_type, test_data in model_data.items():\n",
        "            result_row = {\n",
        "                'model': model_name,\n",
        "                'test_type': test_type if test_type else 'cohesion',  # Fix empty test_type\n",
        "                'accuracy': test_data.get('accuracy', None),\n",
        "                'avg_purity': test_data.get('avg_purity', None),\n",
        "                'passed_tests': test_data.get('passed_tests', None),\n",
        "                'total_tests': test_data.get('total_tests', None),\n",
        "                'avg_time_per_test': test_data.get('avg_time_per_test', None),\n",
        "                'total_time': test_data.get('total_time', None),\n",
        "                'target_achieved': test_data.get('target_achieved', None),\n",
        "                'avg_noise_gap': None,  # Not available in tier1 data\n",
        "                'avg_noise_uncertainty_rate': None,\n",
        "                'avg_noise_ranking_success': None\n",
        "            }\n",
        "            all_results.append(result_row)\n",
        "    \n",
        "    # Process designer noise data\n",
        "    for model_name, model_data in noise_data.items():\n",
        "        for test_type, test_data in model_data.items():\n",
        "            # Extract noise detection metrics\n",
        "            noise_detection = test_data.get('noise_detection', {})\n",
        "            \n",
        "            result_row = {\n",
        "                'model': model_name,\n",
        "                'test_type': 'designer_noise',  # Standardize test type name\n",
        "                'accuracy': test_data.get('accuracy', None),\n",
        "                'avg_purity': None,  # Not applicable for designer noise tests\n",
        "                'passed_tests': test_data.get('successful_tests', None),\n",
        "                'total_tests': test_data.get('total_tests', None),\n",
        "                'avg_time_per_test': test_data.get('average_time_per_test', None),\n",
        "                'total_time': test_data.get('total_time', None),\n",
        "                'target_achieved': None,  # Not applicable\n",
        "                'avg_noise_gap': noise_detection.get('avg_similarity_gap', None),\n",
        "                'avg_noise_uncertainty_rate': noise_detection.get('avg_noise_uncertainty_rate', None),\n",
        "                'avg_noise_ranking_success': noise_detection.get('avg_noise_ranking_success', None)\n",
        "            }\n",
        "            all_results.append(result_row)\n",
        "            break  # Only process gauntlet test for designer noise data\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(all_results)\n",
        "    \n",
        "    # Convert percentage values where appropriate\n",
        "    df['accuracy_pct'] = df['accuracy']\n",
        "    df['avg_purity_pct'] = df['avg_purity']\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Load and combine the data\n",
        "benchmark_df = load_and_combine_benchmark_data()\n",
        "\n",
        "# Display the first few rows to verify successful loading\n",
        "print(\"Successfully loaded and combined benchmark data!\")\n",
        "print(f\"Total rows: {len(benchmark_df)}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(benchmark_df.head(10))\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Methodology\n",
        "\n",
        "Our benchmark consists of three distinct test types, each designed to evaluate different aspects of vision model performance beyond simple accuracy:\n",
        "\n",
        "### 1. The Impostor Test (Uncertainty Detection)\n",
        "This test evaluates a model's ability to distinguish between authentic Rick Owens designs and visually similar work from other designers (Yohji Yamamoto, Ann Demeulemeester). The key metric is the **similarity gap** - the difference between how confidently a model identifies genuine Rick Owens pieces versus impostor designs. A positive gap indicates healthy uncertainty detection, while negative gaps suggest the model is overconfident with noise.\n",
        "\n",
        "### 2. The Family Resemblance Test (Cohesion)\n",
        "This test measures how well models understand the aesthetic DNA of a designer's collection. Models are presented with 100+ images from a single Rick Owens collection and asked to identify the most similar candidates. The **purity percentage** measures how many of the top 20 results actually belong to the target collection, testing the model's grasp of subtle aesthetic consistency.\n",
        "\n",
        "### 3. The Needle in a Haystack Test (Gauntlet)  \n",
        "This test evaluates precision in brand identification by presenting models with one target Rick Owens image hidden among many similar candidates. Success requires the model to rank the exact target image as #1 most similar. This tests both accuracy and the ability to discern minute differences in similar aesthetic contexts.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Key Findings\n",
        "\n",
        "Our comprehensive analysis reveals several critical insights about vision model performance in fashion and design contexts:\n",
        "\n",
        "**SigLIP demonstrates superior uncertainty detection**: While all models achieve similar raw accuracy scores (~90-100%), only SigLIP shows a positive similarity gap (+0.02), indicating it can appropriately distinguish between genuine Rick Owens designs and visually similar impostor work from other designers.\n",
        "\n",
        "**Aesthetic cohesion understanding varies significantly**: SigLIP achieves 63.5% collection purity compared to CLIP (48.8%) and DINOv2 (48.7%), demonstrating a substantially deeper understanding of aesthetic consistency within designer collections.\n",
        "\n",
        "**Processing time reflects model complexity**: SigLIP's superior performance comes at a computational cost, requiring 20-40x longer processing time per test compared to CLIP and DINOv2. This trade-off represents a fundamental challenge in computational aesthetic understanding.\n",
        "\n",
        "**All models struggle with absolute thresholds**: None of the models achieved the target benchmarks (>90% gauntlet accuracy, >80% cohesion purity), suggesting that current vision models still have limitations in highly nuanced aesthetic discrimination tasks.\n",
        "\n",
        "**Uncertainty flagging is model-dependent**: The ability to appropriately flag uncertain cases varies dramatically between models, revealing fundamental differences in how these architectures handle ambiguous visual information.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Analysis & Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 1: Uncertainty Detection (Noise Gap)\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Filter data for designer_noise test type\n",
        "noise_data = benchmark_df[benchmark_df['test_type'] == 'designer_noise'].copy()\n",
        "\n",
        "# Define colors: SigLIP = green, others = red\n",
        "colors = []\n",
        "for model in noise_data['model']:\n",
        "    if model == 'siglip':\n",
        "        colors.append('#28A745')  # Green for SigLIP\n",
        "    else:\n",
        "        colors.append('#DC3545')  # Red for CLIP and DINOv2\n",
        "\n",
        "# Create horizontal bar chart\n",
        "ax = plt.subplot()\n",
        "bars = ax.barh(noise_data['model'], noise_data['avg_noise_gap'], color=colors)\n",
        "\n",
        "# Add vertical dashed line at x=0 (pass/fail threshold)\n",
        "ax.axvline(x=0, color='black', linestyle='--', linewidth=2, alpha=0.7)\n",
        "\n",
        "# Formatting\n",
        "plt.title('Uncertainty Detection: Average Noise Gap by Model', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.xlabel('Similarity Gap (Higher is Better)', fontsize=12)\n",
        "plt.ylabel('Model', fontsize=12)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, value in zip(bars, noise_data['avg_noise_gap']):\n",
        "    width = bar.get_width()\n",
        "    ax.text(width + (0.001 if width >= 0 else -0.001), bar.get_y() + bar.get_height()/2, \n",
        "            f'{value:.3f}', ha='left' if width >= 0 else 'right', va='center', fontweight='bold')\n",
        "\n",
        "# Add gridlines for better readability\n",
        "ax.grid(True, axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Noise Gap Analysis:\")\n",
        "for _, row in noise_data.iterrows():\n",
        "    status = \"✅ PASS\" if row['avg_noise_gap'] > 0 else \"❌ FAIL\"\n",
        "    print(f\"{row['model'].upper()}: {row['avg_noise_gap']:.3f} {status}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Uncertainty Detection Analysis\n",
        "\n",
        "The Noise Gap visualization reveals a critical distinction in model behavior. A **positive gap** indicates that the model assigns higher similarity scores to genuine Rick Owens pieces compared to impostor designs from similar aesthetics (Yohji Yamamoto, Ann Demeulemeester). This is essential for robust brand identification.\n",
        "\n",
        "**Key Insight**: Only SigLIP achieves a positive noise gap (+0.020), demonstrating that it can appropriately distinguish brand identity even when faced with visually similar designs. Both CLIP (-0.015) and DINOv2 (-0.008) show negative gaps, indicating they are **overconfident** with impostor designs and cannot reliably distinguish Rick Owens' unique aesthetic signature.\n",
        "\n",
        "This finding reveals fundamental limitations in current vision models' ability to handle aesthetic ambiguity, highlighting areas for future research in uncertainty quantification for visual understanding tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 2: Aesthetic Cohesion (Purity %)\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Filter data for cohesion test type\n",
        "cohesion_data = benchmark_df[benchmark_df['test_type'] == 'cohesion'].copy()\n",
        "\n",
        "# Create vertical bar chart with professional blue palette\n",
        "blue_palette = ['#1f4e79', '#4A90E2', '#7bb3f0']  # Different shades of blue\n",
        "ax = plt.subplot()\n",
        "bars = ax.bar(cohesion_data['model'], cohesion_data['avg_purity'], color=blue_palette)\n",
        "\n",
        "# Formatting\n",
        "plt.title('Aesthetic Cohesion: Average Collection Purity by Model', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.ylabel('Collection Purity (%)', fontsize=12)\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for bar, value in zip(bars, cohesion_data['avg_purity']):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "            f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Set y-axis to start from 0 and add some headroom\n",
        "ax.set_ylim(0, max(cohesion_data['avg_purity']) * 1.1)\n",
        "\n",
        "# Add gridlines for better readability\n",
        "ax.grid(True, axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Collection Purity Analysis:\")\n",
        "for _, row in cohesion_data.iterrows():\n",
        "    target_status = \"✅ TARGET MET\" if row['avg_purity'] >= 80 else \"❌ BELOW TARGET\"\n",
        "    print(f\"{row['model'].upper()}: {row['avg_purity']:.1f}% {target_status}\")\n",
        "\n",
        "print(f\"\\nTarget: >80% collection purity\")\n",
        "print(f\"Best performer: {cohesion_data.loc[cohesion_data['avg_purity'].idxmax(), 'model'].upper()} ({cohesion_data['avg_purity'].max():.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Aesthetic Cohesion Analysis\n",
        "\n",
        "The Collection Purity metric measures how well models understand the subtle aesthetic consistency within a designer's work. When shown images from a single Rick Owens collection, models must identify which other candidates belong to the same aesthetic family.\n",
        "\n",
        "**Key Insight**: SigLIP achieves 63.5% purity compared to CLIP (48.8%) and DINOv2 (48.7%), demonstrating significantly superior understanding of aesthetic cohesion. This 15-point advantage suggests that SigLIP has developed a more nuanced grasp of the visual \"DNA\" that makes a collection coherent.\n",
        "\n",
        "While none of the models reach the aspirational 80% target, SigLIP's performance indicates it can capture aesthetic relationships that go beyond surface-level visual similarity. This demonstrates meaningful progress in computational understanding of design language and aesthetic consistency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 3: Processing Time\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Calculate average processing time per model across all test types\n",
        "avg_times = benchmark_df.groupby('model')['avg_time_per_test'].mean().reset_index()\n",
        "avg_times = avg_times.sort_values('avg_time_per_test')\n",
        "\n",
        "# Create horizontal bar chart with neutral gray palette\n",
        "gray_palette = ['#6c757d', '#495057', '#343a40']  # Different shades of gray\n",
        "ax = plt.subplot()\n",
        "bars = ax.barh(avg_times['model'], avg_times['avg_time_per_test'], color=gray_palette)\n",
        "\n",
        "# Formatting\n",
        "plt.title('Performance: Average Processing Time per Test', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.xlabel('Time (seconds)', fontsize=12)\n",
        "plt.ylabel('Model', fontsize=12)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, value in zip(bars, avg_times['avg_time_per_test']):\n",
        "    width = bar.get_width()\n",
        "    ax.text(width + 0.5, bar.get_y() + bar.get_height()/2, \n",
        "            f'{value:.1f}s', ha='left', va='center', fontweight='bold')\n",
        "\n",
        "# Add gridlines for better readability\n",
        "ax.grid(True, axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Processing Time Analysis:\")\n",
        "for _, row in avg_times.iterrows():\n",
        "    print(f\"{row['model'].upper()}: {row['avg_time_per_test']:.1f} seconds per test\")\n",
        "\n",
        "# Calculate performance ratios\n",
        "fastest = avg_times['avg_time_per_test'].min()\n",
        "print(f\"\\nPerformance Ratios (vs fastest):\")\n",
        "for _, row in avg_times.iterrows():\n",
        "    ratio = row['avg_time_per_test'] / fastest\n",
        "    print(f\"{row['model'].upper()}: {ratio:.1f}x slower than fastest\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
